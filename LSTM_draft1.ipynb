{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from Bio import SeqIO\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def load_test_sets(filename):\n",
    "    go_data = sio.loadmat(filename, squeeze_me=True)\n",
    "    go_terms = go_data['goTerm_labels'] # names of gene ontology function terms\n",
    "    train_annotations = np.asarray(go_data['trainProts_label'].todense()) # training set of function annotations\n",
    "    valid_annotations = np.asarray(go_data['validProts_label'].todense()) # valid \"\" \"\"\n",
    "    test_annotations = np.asarray(go_data['testProts_label'].todense()) # test \"\" \"\"\n",
    "    train_inds = go_data['trainProts']\n",
    "    train_inds = train_inds - 1\n",
    "    valid_inds = go_data['validProts']\n",
    "    valid_inds = valid_inds - 1\n",
    "    test_inds = go_data['testProts']\n",
    "    test_inds = test_inds - 1 # subtract 1 for matlab index conversion into python\n",
    "\n",
    "    return train_inds, valid_inds, test_inds, train_annotations, valid_annotations, test_annotations, go_terms\n",
    "\n",
    "def load_FASTA(filename):\n",
    "    \"\"\" Loads fasta file and returns a list of the Bio SeqIO records \"\"\"\n",
    "    infile = open(filename)\n",
    "    full_entries = list(SeqIO.parse(infile, 'fasta'))\n",
    "    sequences = [str(entry.seq) for entry in full_entries]\n",
    "    names = [str(entry.id) for entry in full_entries]\n",
    "\n",
    "    return sequences, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load human sequences and create train/dev/test sets, create lengths for FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human_sequences, human_protein_names = load_FASTA('../data/human_sequences.fasta')\n",
    "human_train_idx, human_valid_idx, human_test_idx, human_train_labels, human_valid_labels, \\\n",
    "    human_test_labels, human_GO_terms = load_test_sets('../data/human_annotations_temporal_holdout.mat')\n",
    "\n",
    "# Create train, validation, and test sets from the full list of human proteins\n",
    "human_train_sequences = [human_sequences[i] for i in human_train_idx]\n",
    "human_valid_sequences = [human_sequences[i] for i in human_valid_idx]\n",
    "human_test_sequences = [human_sequences[i] for i in human_test_idx]\n",
    "\n",
    "# Convert corresponding labels for train, validation, and test sets from the full list of human proteins.\n",
    "human_train_labels = torch.from_numpy(human_train_labels).type(torch.LongTensor)\n",
    "human_valid_labels = torch.from_numpy(human_valid_labels).type(torch.LongTensor)\n",
    "human_test_labels = torch.from_numpy(human_test_labels).type(torch.LongTensor)\n",
    "\n",
    "# Create lengths for sequence representation averaging in FastText\n",
    "human_train_lengths = torch.LongTensor([len(human_train_sequences[i]) for i in range(len(human_train_sequences))])\n",
    "human_valid_lengths = torch.LongTensor([len(human_valid_sequences[i]) for i in range(len(human_valid_sequences))])\n",
    "human_test_lengths = torch.LongTensor([len(human_test_sequences[i]) for i in range(len(human_test_sequences))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load yeast sequences and create train/dev/test sets, create lengths for FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load yeast sequences and training data\n",
    "yeast_sequences, yeast_protein_names = load_FASTA('../data/yeast_sequences.fasta')\n",
    "yeast_train_idx, yeast_valid_idx, yeast_test_idx, yeast_train_labels, yeast_valid_labels, \\\n",
    "    yeast_test_labels, yeast_GO_terms = load_test_sets('../data/yeast_MF_temporal_holdout.mat')\n",
    "\n",
    "# Create train, validation, and test sets from the full list of yeast proteins\n",
    "yeast_train_sequences = [yeast_sequences[i] for i in yeast_train_idx]\n",
    "yeast_valid_sequences = [yeast_sequences[i] for i in yeast_valid_idx]\n",
    "yeast_test_sequences = [yeast_sequences[i] for i in yeast_test_idx]\n",
    "\n",
    "# Convert corresponding labels for train, validation, and test sets from the full list of yeast proteins.\n",
    "yeast_train_labels = torch.from_numpy(yeast_train_labels).type(torch.LongTensor)\n",
    "yeast_valid_labels = torch.from_numpy(yeast_valid_labels).type(torch.LongTensor)\n",
    "yeast_test_labels = torch.from_numpy(yeast_test_labels).type(torch.LongTensor)\n",
    "\n",
    "# Create lengths for sequence representation averaging in FastText\n",
    "yeast_train_lengths = torch.LongTensor([len(yeast_train_sequences[i]) for i in range(len(yeast_train_sequences))])\n",
    "yeast_valid_lengths = torch.LongTensor([len(yeast_valid_sequences[i]) for i in range(len(yeast_valid_sequences))])\n",
    "yeast_test_lengths = torch.LongTensor([len(yeast_test_sequences[i]) for i in range(len(yeast_test_sequences))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed amino-acid chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each amino-acid string becomes an NxD entry in a tensor, where N is the number of \n",
    "# amino-acid strings and D is the length of the longest chain in the set. \n",
    "\n",
    "ConvertCharToInt = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'H':8, 'I':9, 'J':10,\n",
    "                   'K':11, 'L':12, 'M':13, 'N':14, 'O':15, 'P':16, 'Q':17, 'R':18, 'S':19,\n",
    "                   'T':20, 'U':21, 'V':22, 'W':23, 'X':24, 'Y':25, 'Z':26}\n",
    "\n",
    "def vectorize_AAs(string):\n",
    "    '''This function takes an amino-acid string as input and outputs a vector of integers, with each\n",
    "    integer representing one amino acid.\n",
    "    \n",
    "    For example, 'BACEA' is converted to [2, 1, 3, 5, 1]\n",
    "    '''\n",
    "    character_list = list(string) #converts 'BACEA' to ['B','A','C','E','A]\n",
    "    for i in range(len(character_list)):\n",
    "        character_list[i] = ConvertCharToInt[character_list[i]] #convert the character to a number\n",
    "    return character_list\n",
    "\n",
    "def AddZeros(vector, max_length):\n",
    "    '''This function adds the necessary number of zeros and returns an array'''\n",
    "    #max_length = length of longest vector in the batch\n",
    "    #oldvector = initial vector for that amino-acid chain (in integers)\n",
    "    diff = max_length - len(vector)\n",
    "    if diff>0:\n",
    "        ZerosToAdd = np.zeros(diff)\n",
    "        vector.extend(ZerosToAdd)\n",
    "    return vector \n",
    "\n",
    "def TransformAAsToTensor(ListOfSequences):\n",
    "    '''This function takes as input a list of amino acid strings and creates a tensor matrix\n",
    "    of dimension NxD, where N is the number of strings and D is the length of the longest AA chain\n",
    "    \n",
    "    \"ListOfSequences\" can be training, validation, or test sets\n",
    "    '''\n",
    "    #find longest amino-acid sequence\n",
    "    max_length = len(max(ListOfSequences, key=len))\n",
    "    Sequences = ListOfSequences.copy() \n",
    "    for AA in range(len(Sequences)): #for each amino-acid sequence\n",
    "        Sequences[AA] = vectorize_AAs(Sequences[AA])\n",
    "        Sequences[AA] = AddZeros(Sequences[AA], max_length)\n",
    "    NewTensor = torch.from_numpy(np.array(Sequences))\n",
    "    return NewTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human_train_vectors = TransformAAsToTensor(human_train_sequences)\n",
    "human_valid_vectors = TransformAAsToTensor(human_valid_sequences)\n",
    "human_test_vectors = TransformAAsToTensor(human_test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yeast_train_vectors = TransformAAsToTensor(yeast_train_sequences)\n",
    "yeast_valid_vectors = TransformAAsToTensor(yeast_valid_sequences)\n",
    "yeast_test_vectors = TransformAAsToTensor(yeast_test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Hyperparameter setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Get batch data method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def batch_iter(TrainSeqs, yTrain, TrainSeqsLength, batch_size):\n",
    "    start = -1 * batch_size\n",
    "    dataset_size = TrainSeqs.size()[0]\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch_indices_tensor = torch.LongTensor(batch_indices)\n",
    "        batch_train = TrainSeqs[batch_indices_tensor].type(torch.LongTensor)\n",
    "        batch_train_labels = yTrain[batch_indices_tensor]\n",
    "        length_batch = TrainSeqsLength[batch_indices_tensor]\n",
    "        yield [Variable(batch_train), Variable(batch_train_labels), Variable(length_batch)]  \n",
    "        \n",
    "        \n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "        else:\n",
    "            continue\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) FastText class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    \"\"\"\n",
    "    FastText model\n",
    "    \"\"\"\n",
    "       \n",
    "    def __init__(self, vocab_size, emb_dim, num_labels):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(FastText, self).__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.embed = nn.Embedding(vocab_size+1, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,num_labels)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        data = data.type(torch.LongTensor)\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out = out / length.view(-1,1).float()\n",
    "            \n",
    "        out = self.linear(out)\n",
    "        return nn.functional.sigmoid(out)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B) LSTM class:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model\n",
    "    \"\"\"  \n",
    "    def __init__(self, vocab_size, emb_dim, hidden_size, num_labels, batch_size):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear_f = nn.Linear(emb_dim + hidden_size, hidden_size)\n",
    "        self.linear_i = nn.Linear(emb_dim + hidden_size, hidden_size)\n",
    "        self.linear_ctilde = nn.Linear(emb_dim + hidden_size, hidden_size)\n",
    "        self.linear_o = nn.Linear(emb_dim + hidden_size, hidden_size)\n",
    "    \n",
    "        self.decoder = nn.Linear(hidden_size, num_labels)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, data, hidden, c):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        data = data.type(torch.LongTensor)\n",
    "        emb = self.embed(data)\n",
    "        embs = torch.chunk(emb, emb.size()[1], 1)\n",
    "        \n",
    "        def step(emb, hid, c_t):\n",
    "            combined = torch.cat((hid,emb),1)\n",
    "            f = F.sigmoid(self.linear_f(combined))\n",
    "            i = F.sigmoid(self.linear_i(combined))\n",
    "            c_tilde = F.tanh(self.linear_ctilde(combined))\n",
    "            c_t = f*c_t + i*c_tilde\n",
    "            o = F.sigmoid(self.linear_o(combined))\n",
    "            hid = o * F.tanh(c_t)\n",
    "            return hid, c_t\n",
    "        \n",
    "        for i in range(len(embs)):\n",
    "            hidden, c = step(embs[i].squeeze(), hidden, c)\n",
    "        output = self.decoder(hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(self.batch_size, self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.batch_size, self.hidden_size))\n",
    "        return h0, c0\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_f, self.linear_i, self.linear_ctilde, self.linear_o]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,average_precision_score\n",
    "\n",
    "def round_manual(data, threshold):\n",
    "    return (data >= threshold).astype(int)\n",
    "\n",
    "def calculate_accuracy(predicted, actuals, num_labels):\n",
    "    \"\"\"\n",
    "    @param predicted: data type = Variable\n",
    "    @param actuals: data type = Variable\n",
    "    @param num_labels: no of go terms\n",
    "    @return: accuracy measure\n",
    "    \"\"\"\n",
    "    predicted = np.round(predicted.data.numpy())\n",
    "    total_predictions = actuals.size()[0]\n",
    "    accuracy = np.sum(predicted==actuals.data.numpy())/(total_predictions*num_labels)\n",
    "    return accuracy\n",
    "\n",
    "def m_tau(predictions):\n",
    "    return len([np.sum(i) for i in predictions if np.sum(i)!=0])\n",
    "\n",
    "def n_e(predictions):\n",
    "    return predictions.shape[0]\n",
    "\n",
    "def calculate_recall_precision(predicted, actual):\n",
    "    '''\n",
    "    Overall, this function calculates the recall and precision of the validation set proteins.\n",
    "    The function FIRST calculates the precision and recall values of INDIVIDUAL proteins. \n",
    "    It then takes the mean average of these values to get \"dataset-level\" precision and recall.\n",
    "    '''\n",
    "    \n",
    "    PositivesPerRow = actual.numpy().sum(axis=1) #number of functions for each protein\n",
    "    PosPredictionsPerRow = predicted.sum(axis=1) #number of predictions for each protein\n",
    "    TPs = np.multiply(actual.numpy(), predicted) #element-wise multiplication: 1 if TP, else 0\n",
    "    TPsPerRow = TPs.sum(axis=1) #number of true positives for each protein\n",
    "    \n",
    "    #PrecisionPerRow (Protein) - if protein has 0 positive predictions, the protein's precision = 0.\n",
    "    #Else, the protein's precision = TPs/PositivePreds\n",
    "    PrecisionPerRow = np.where(PosPredictionsPerRow == 0, 0, TPsPerRow/PosPredictionsPerRow)\n",
    "    RecallPerRow = np.where(PositivesPerRow==0, 0, TPsPerRow/PositivesPerRow) #Recall per Protein\n",
    "    \n",
    "    #RecallScore = average of individual protein recall scores\n",
    "    RecallScore = sum(RecallPerRow)/len(RecallPerRow) #denominator is non-zero\n",
    "    \n",
    "    #PrecisionScore = average of CERTAIN individual protein precision scores (see line below)\n",
    "    #Only consider rows with at least one predicted Go-Term.\n",
    "    #Note that some proteins can have Precision=0 but still have predictions.\n",
    "    if sum(PrecisionPerRow)>0:\n",
    "        PrecisionScore = sum(PrecisionPerRow)/len([x for x in PosPredictionsPerRow if x!=0]) \n",
    "    else:\n",
    "        PrecisionScore = 0\n",
    "    return RecallScore, PrecisionScore\n",
    "    \n",
    "    \n",
    "def F_score(predicted, actuals):\n",
    "    \"\"\"\n",
    "    @param predicted: data type = Variable\n",
    "    @param actuals: data type = Variable\n",
    "    @return: Maximum f score over all values of tau and the corresponding tau threshold\n",
    "    \"\"\"\n",
    "    f_max, optimal_threshold, optimal_precision, optimal_recall = 0, 0, 0, 0\n",
    "    for threshold in [i/100 for i in range(1,100)]:\n",
    "        predicted_tau = round_manual(predicted.data.numpy(), threshold)\n",
    "        recall_score, precision_score = calculate_recall_precision(predicted_tau, actuals)\n",
    "        \n",
    "        if recall_score==0 and precision_score==0:\n",
    "            output = 0\n",
    "        else:\n",
    "            output = ((2*precision_score*recall_score) / (precision_score + recall_score))\n",
    "        if output > f_max:\n",
    "            f_max = output\n",
    "            optimal_threshold = threshold\n",
    "            optimal_precision = precision_score\n",
    "            optimal_recall = recall_score\n",
    "    \n",
    "    return f_max, optimal_threshold, optimal_precision, optimal_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Early stop condition and training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def early_stop(val_loss_history, t=10, required_progress=0.001):\n",
    "    \"\"\"\n",
    "    Stop the training if there is no non-trivial progress in k steps\n",
    "    @param val_acc_history: a list contains all the historical validation acc\n",
    "    @param required_progress: the next acc should be higher than the previous by \n",
    "        at least required_progress amount to be non-trivial\n",
    "    @param t: number of training steps \n",
    "    @return: a boolean indicates if the model should earily stop\n",
    "    \"\"\"    \n",
    "    cnt = 0 # initialize the count --> to store count of cases where difference in\n",
    "                                    #  accuracy is less than required progress.\n",
    "    \n",
    "    if(len(val_loss_history) > 0): # if list has size > 0 \n",
    "        for i in range(t): # start the loop\n",
    "            index = len(val_loss_history) - (i+1) # start from the last term in list and move to the left\n",
    "            if (index >= 1): # to check if index != 0 --> else we can't compare to previous value\n",
    "                if ((val_loss_history[index-1] - val_loss_history[index]) < required_progress):\n",
    "                    cnt += 1 # increase the count value\n",
    "                else:\n",
    "                    break # break if difference is greater \n",
    "    \n",
    "    if(cnt != t): # if count is equal to t, return True\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "\n",
    "def get_full_LSTM_validation_output(model, dev_iter):\n",
    "    '''This function gathers the validation outputs under the LSTM model.\n",
    "    The function was necessary because in our LSTM, we must define a batch_size.\n",
    "    '''\n",
    "    model.eval()\n",
    "    for i, (valid_data, valid_labels, length_batch) in enumerate(data_iter):\n",
    "        hidden, c_t = model.init_hidden()\n",
    "        output, hidden = model(valid_data, hidden, c_t)\n",
    "        if i == 0:\n",
    "            valid_output = output\n",
    "        else:\n",
    "            valid_output = torch.cat((valid_output, output), 0)\n",
    "        print(valid_output)\n",
    "    return valid_output\n",
    "\n",
    "\n",
    "def train_test(valid_sequences, valid_label, valid_length, num_epochs, optimizer, data_iter,\n",
    "               model, training_length, lstm=False):\n",
    "    losses = []\n",
    "    total_batches = int(training_length/ batch_size)\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        stop_training = False\n",
    "        for i, (train_data, train_labels, length_batch) in enumerate(data_iter):\n",
    "            \n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            if lstm:\n",
    "                hidden, c_t = model.init_hidden()\n",
    "                outputs, hidden = model(train_data, hidden, c_t)\n",
    "            else:\n",
    "                outputs = model(train_data, length_batch)\n",
    "\n",
    "            loss = criterion(outputs, train_labels.float())\n",
    "            losses.append(loss.data[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            if lstm:\n",
    "                val_outputs = get_full_LSTM_validation_output(model, dev_iter)\n",
    "            else:\n",
    "                val_outputs = model(Variable(valid_sequences), Variable(valid_length))\n",
    "            \n",
    "            f_score,threshold,precision,recall = F_score(val_outputs, valid_label)\n",
    "            validation_loss = criterion(val_outputs, Variable(valid_label).float()).data[0]\n",
    "            validation_loss_history.append(validation_loss)\n",
    "            stop_training = early_stop(validation_loss_history)\n",
    "\n",
    "            if stop_training:\n",
    "                print(\"earily stop triggered\")\n",
    "                break\n",
    "            \n",
    "            if (i+1) % batch_size == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train loss: {}, Validation loss for batch: {},\\nValid F_Score: {}, Threshold: {}, Valid Precision: {}, Valid Recall: {}'\\\n",
    "                      .format(epoch, num_epochs, i+1, total_batches, np.mean(losses)/(total_batches*epoch), \\\n",
    "                        validation_loss, f_score, threshold, precision,recall))\n",
    "        \n",
    "        if stop_training == True:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Prediction Test Data performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_on_test_set(model, test_input_seq, test_seq_length, test_output_labels, num_labels):\n",
    "    test_input_seq = Variable(test_input_seq)\n",
    "    test_seq_length = Variable(test_seq_length)\n",
    "    test_output_labels = Variable(test_output_labels)\n",
    "    predicted = model(test_input_seq, test_seq_length)\n",
    "    accuracy_on_test_set = calculate_accuracy(predicted, test_output_labels, num_labels)\n",
    "    return accuracy_on_test_set\n",
    "\n",
    "def FScore_on_test_set(model, test_input_seq, test_seq_length, test_output_labels, num_labels):\n",
    "    test_input_seq = Variable(test_input_seq)\n",
    "    test_seq_length = Variable(test_seq_length)\n",
    "    #test_output_labels = Variable(test_output_labels)\n",
    "    predicted = model(test_input_seq, test_seq_length)\n",
    "    fmax, optimal_threshold, optimal_precision, optimal_recall = F_score(predicted, test_output_labels)\n",
    "    return fmax, optimal_threshold, optimal_precision, optimal_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Model training and test performance - FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "vocab_size = 26 # number words in the vocabulary base\n",
    "emb_dim = 20 # dimension for n-gram embedding\n",
    "num_epochs = 500 # number epoch to train\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART I - Human Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_size = len(human_train_sequences)\n",
    "num_labels = human_GO_terms.shape[0] #147\n",
    "\n",
    "model = FastText(vocab_size, emb_dim, num_labels)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brenton/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/Brenton/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:38: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/500], Step: [20/251], Train loss: 0.003524817069213229, Validation loss for batch: 0.8964088559150696,\n",
      "Valid F_Score: 0.08513572538260669, Threshold: 0.34, Valid Precision: 0.045055269140362016, Valid Recall: 0.7710466904242627\n",
      "Epoch: [1/500], Step: [40/251], Train loss: 0.0033185003050769943, Validation loss for batch: 0.8582369685173035,\n",
      "Valid F_Score: 0.08854467958407468, Threshold: 0.29, Valid Precision: 0.04726054428101161, Valid Recall: 0.7001980228882487\n",
      "Epoch: [1/500], Step: [60/251], Train loss: 0.0031935830040281986, Validation loss for batch: 0.8362264037132263,\n",
      "Valid F_Score: 0.10121416152836812, Threshold: 0.37, Valid Precision: 0.06139901522650984, Valid Recall: 0.2879210333658487\n",
      "earily stop triggered\n",
      "Test Data F-Score for yeast protein prediction is 0.0500766960538 \n",
      "Test Precision: 0.0286021008083 \n",
      "Test Recall: 0.2009537132\n"
     ]
    }
   ],
   "source": [
    "data_iter = batch_iter(human_train_vectors, human_train_labels, human_train_lengths, batch_size)\n",
    "\n",
    "# Model Training\n",
    "train_test(human_valid_vectors, human_valid_labels, human_valid_lengths, \\\n",
    "           num_epochs, optimizer, data_iter, model, data_size) \n",
    "\n",
    "FScore,_,Precision,Recall = FScore_on_test_set(model, human_test_vectors, human_test_lengths,human_test_labels, num_labels)\n",
    "\n",
    "# Prediction on test set\n",
    "print(\"Test Data F-Score for yeast protein prediction is\", FScore, '\\nTest Precision:',Precision,\n",
    "      '\\nTest Recall:',Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART II - Yeast Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brenton/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:39: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/Brenton/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:38: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earily stop triggered\n",
      "Test Data F-Score for yeast protein prediction is 0.167035551632 \n",
      "Test Precision: 0.0959770008899 \n",
      "Test Recall: 0.643361492876\n"
     ]
    }
   ],
   "source": [
    "data_size = len(yeast_train_sequences) #3447\n",
    "num_labels = yeast_GO_terms.shape[0] #26\n",
    "\n",
    "model = FastText(vocab_size, emb_dim, num_labels)\n",
    "data_iter = batch_iter(yeast_train_vectors, yeast_train_labels, yeast_train_lengths, batch_size)\n",
    "\n",
    "# Model Training\n",
    "train_test(yeast_valid_vectors, yeast_valid_labels, yeast_valid_lengths, \\\n",
    "           num_epochs, optimizer, data_iter, model, data_size)\n",
    "\n",
    "FScore,_,Precision,Recall = FScore_on_test_set(model, yeast_test_vectors, yeast_test_lengths,yeast_test_labels, num_labels)\n",
    "\n",
    "# Prediction on test set\n",
    "print(\"Test Data F-Score for yeast protein prediction is\", FScore, '\\nTest Precision:',Precision,\n",
    "      '\\nTest Recall:',Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B) Model training and test performance - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I - Human Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "vocab_size = 26 # number words in the vocabulary base\n",
    "emb_dim = 20 # dimension for n-gram embedding\n",
    "num_epochs = 500 # number epoch to train\n",
    "batch_size = 40\n",
    "\n",
    "#New parameters\n",
    "hidden_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_size = len(human_train_sequences)\n",
    "num_labels = human_GO_terms.shape[0] #147\n",
    "\n",
    "model = LSTM(vocab_size, emb_dim, hidden_size, num_labels, batch_size)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "data_iter = batch_iter(human_train_vectors, human_train_labels, human_train_lengths, batch_size)\n",
    "dev_iter = eval_iter(human_valid_vectors, human_valid_labels, batch_size)\n",
    "\n",
    "# Model Training\n",
    "train_test(human_valid_vectors, human_valid_labels, human_valid_lengths, \\\n",
    "           num_epochs, optimizer, data_iter, model, data_size, lstm=True) \n",
    "\n",
    "FScore,_,Precision,Recall = FScore_on_test_set(model, human_test_vectors, human_test_lengths,human_test_labels, num_labels)\n",
    "\n",
    "# Prediction on test set\n",
    "print(\"Test Data F-Score for human protein prediction is\", FScore, '\\nTest Precision:',Precision,\n",
    "      '\\nTest Recall:',Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II - Yeast Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6995\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 40x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 80x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 120x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 160x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 200x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 240x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 280x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 320x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 360x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 400x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 440x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 480x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 520x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 560x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 600x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 640x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 680x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 720x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 760x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 800x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 840x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 880x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 920x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 960x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1000x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1040x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1080x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1120x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1160x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1200x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1240x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1280x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1320x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1360x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1400x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1440x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1480x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1520x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1560x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1600x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1640x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1680x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1720x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1760x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1800x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1840x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1880x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1920x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 1960x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2000x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2040x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2080x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2120x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2160x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2200x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2240x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2280x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2320x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2360x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2400x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2440x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2480x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2520x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2560x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2600x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2640x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2680x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2720x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2760x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2800x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2840x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2880x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2920x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 2960x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3000x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3040x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3080x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3120x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3160x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3200x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3240x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3280x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3320x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3360x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3400x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3440x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3480x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3520x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3560x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3600x26]\n",
      "\n",
      "Variable containing:\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "          ...             ⋱             ...          \n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "-0.0177 -0.0970 -0.1104  ...   0.0989  0.0472  0.0030\n",
      "[torch.FloatTensor of size 3640x26]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d559343476bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Model Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myeast_valid_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myeast_valid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myeast_valid_lengths\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mFScore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFScore_on_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myeast_test_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myeast_test_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myeast_test_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-6410bc677bf7>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(valid_sequences, valid_label, valid_length, num_epochs, optimizer, data_iter, dev_iter, model, training_length, lstm)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mval_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_full_LSTM_validation_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mval_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-6410bc677bf7>\u001b[0m in \u001b[0;36mget_full_LSTM_validation_output\u001b[0;34m(model, dev_iter)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mvalid_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Brenton/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0c736a4f3d26>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, hidden, c)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0c736a4f3d26>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(emb, hid, c_t)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mc_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_ctilde\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc_tilde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_o\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mhid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Brenton/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Brenton/anaconda/lib/python3.5/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Brenton/anaconda/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Brenton/anaconda/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Brenton/anaconda/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Brenton/anaconda/lib/python3.5/site-packages/torch/autograd/_functions/blas.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInplaceFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_size = len(yeast_train_sequences) #3447\n",
    "num_labels = yeast_GO_terms.shape[0] #26\n",
    "\n",
    "model = LSTM(vocab_size, emb_dim, hidden_size, num_labels, batch_size)\n",
    "data_iter = batch_iter(yeast_train_vectors, yeast_train_labels, yeast_train_lengths, batch_size)\n",
    "dev_iter = batch_iter(yeast_valid_vectors, yeast_valid_labels, yeast_valid_lengths, batch_size)\n",
    "\n",
    "# Model Training\n",
    "train_test(yeast_valid_vectors, yeast_valid_labels, yeast_valid_lengths, \\\n",
    "           num_epochs, optimizer, data_iter, dev_iter, model, data_size, lstm=True)\n",
    "\n",
    "FScore,_,Precision,Recall = FScore_on_test_set(model, yeast_test_vectors, yeast_test_lengths,yeast_test_labels, num_labels)\n",
    "\n",
    "# Prediction on test set\n",
    "print(\"Test Data F-Score for yeast protein prediction is\", FScore, '\\nTest Precision:',Precision,\n",
    "      '\\nTest Recall:',Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
