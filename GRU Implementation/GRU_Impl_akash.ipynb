{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training prots: 9751\n",
      "Number of validation prots: 3871\n",
      "Number of testing prots: 1647\n",
      "Number of training prots: 3447\n",
      "Number of validation prots: 963\n",
      "Number of testing prots: 206\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from Bio import SeqIO\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from evaluation_metrics import *\n",
    "from data_processing import *\n",
    "\n",
    "#Human Sequences\n",
    "fasta = '../../../data/human_sequences.fasta'\n",
    "test_set_file = '../../../data/human_annotations_temporal_holdout.mat'\n",
    "\n",
    "sequences, names = load_FASTA(fasta)\n",
    "train_inds, valid_inds, test_inds, y_trainHuman, y_validHuman, y_testHuman, go_termsHuman = load_test_sets(test_set_file)\n",
    "\n",
    "train_seqsHuman = [sequences[i] for i in train_inds]\n",
    "print('Number of training prots: ' + str(len(train_seqsHuman)))\n",
    "valid_seqsHuman = [sequences[i] for i in valid_inds]\n",
    "print('Number of validation prots: ' + str(len(valid_seqsHuman)))\n",
    "test_seqsHuman = [sequences[i] for i in test_inds]\n",
    "print('Number of testing prots: ' + str(len(test_seqsHuman)))\n",
    "\n",
    "#Yeast sequences\n",
    "fasta = '../../../data/yeast_sequences.fasta'\n",
    "test_set_file = '../../../data/yeast_MF_temporal_holdout.mat'\n",
    "\n",
    "sequences, names = load_FASTA(fasta)\n",
    "train_inds, valid_inds, test_inds, y_trainYeast, y_validYeast, y_testYeast, go_termsYeast = load_test_sets(test_set_file)\n",
    "\n",
    "train_seqsYeast = [sequences[i] for i in train_inds]\n",
    "print('Number of training prots: ' + str(len(train_seqsYeast)))\n",
    "valid_seqsYeast = [sequences[i] for i in valid_inds]\n",
    "print('Number of validation prots: ' + str(len(valid_seqsYeast)))\n",
    "test_seqsYeast = [sequences[i] for i in test_inds]\n",
    "print('Number of testing prots: ' + str(len(test_seqsYeast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yTrainYeast = torch.from_numpy(y_trainYeast).type(torch.LongTensor)\n",
    "yValidYeast = torch.from_numpy(y_validYeast).type(torch.LongTensor)\n",
    "yTestYeast = torch.from_numpy(y_testYeast).type(torch.LongTensor)\n",
    "\n",
    "yTrainHuman = torch.from_numpy(y_trainHuman).type(torch.LongTensor)\n",
    "yValidHuman = torch.from_numpy(y_validHuman).type(torch.LongTensor)\n",
    "yTestHuman = torch.from_numpy(y_testHuman).type(torch.LongTensor)\n",
    "\n",
    "#yTrainYeast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Length of train, valid and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 2 #value for kmers\n",
    "\n",
    "train_seqsHuman_length = sequence_lengths_with_kmers(train_seqsHuman, k) \n",
    "valid_seqsHuman_length = sequence_lengths_with_kmers(valid_seqsHuman, k)\n",
    "test_seqsHuman_length = sequence_lengths_with_kmers(test_seqsHuman, k)\n",
    "train_seqsYeast_length = sequence_lengths_with_kmers(train_seqsYeast, k)\n",
    "valid_seqsYeast_length = sequence_lengths_with_kmers(valid_seqsYeast, k)\n",
    "test_seqsYeast_length = sequence_lengths_with_kmers(test_seqsYeast, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize all amino-acid chains in the list \n",
    "#### Each amino-acid string becomes one row in a tensor object.\n",
    "#### This tensor object has dimension NxD, where N is the number of amino-acid strings and D is the length of the longest chain in the set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get kmers (Can take upto 3-4 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_mers_human = get_k_mers(train_seqsHuman, valid_seqsHuman, test_seqsHuman, k)\n",
    "k_mers_yeast = get_k_mers(train_seqsYeast, valid_seqsYeast, test_seqsYeast, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runs quickly for Yeast, about 2 minutes for Human data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TrainSeqsYeast = TransformAAsToTensor_with_kmers(train_seqsYeast, k, k_mers_yeast)\n",
    "ValidSeqsYeast = TransformAAsToTensor_with_kmers(valid_seqsYeast, k, k_mers_yeast)\n",
    "TestSeqsYeast = TransformAAsToTensor_with_kmers(test_seqsYeast, k, k_mers_yeast)\n",
    "#TestSeqsYeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This can take a while for k >= 2\n",
    "TrainSeqsHuman = TransformAAsToTensor_with_kmers(train_seqsHuman, k, k_mers_human)\n",
    "ValidSeqsHuman = TransformAAsToTensor_with_kmers(valid_seqsHuman, k, k_mers_human)\n",
    "TestSeqsHuman = TransformAAsToTensor_with_kmers(test_seqsHuman, k, k_mers_human)\n",
    "#TestSeqsHuman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "#vocab_size = 26 # number words in the vocabulary base\n",
    "vocab_size = len(acid_dict) + len(k_mers_human)\n",
    "emb_dim = 50 # dimension for n-gram embedding\n",
    "num_epochs = 5 # number epoch to train\n",
    "batch_size = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Get batch data method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) GRU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_GRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU model\n",
    "    \"\"\"\n",
    "       \n",
    "    def __init__(self, vocab_size, emb_dim, num_labels, hidden_size, n_layers=1, dropout=0.1, is_bidirectional = False):\n",
    "       \n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(RNN_GRU, self).__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.num_directions = 1 # it is 2 if the rnn is bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed = nn.Embedding(vocab_size+1, emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_size, n_layers, dropout, bidirectional=is_bidirectional)\n",
    "        self.linear = nn.Linear(self.num_directions*hidden_size, num_labels)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_seqs):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        embedded = self.embed(input_seqs) # size = (max_length, batch_size, embed_size)\n",
    "        #packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # size = (max_length * batch_size, embed_size)\n",
    "        \n",
    "        hidden = None\n",
    "        outputs, hidden = self.gru(embedded, hidden) # outputs are supposed to be probability distribution right?\n",
    "        #print(outputs.size(), hidden.size())\n",
    "        #outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        \n",
    "        if self.is_bidirectional == True:\n",
    "            outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "            \n",
    "        last_hidden = self.dropout(outputs[-1,:,:].squeeze())            \n",
    "        output_probability = torch.nn.functional.sigmoid(self.linear(last_hidden))\n",
    "        \n",
    "        return output_probability # size : (batch_size, num_labels)\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def early_stop(val_acc_history, t=2, required_progress=0.00001):\n",
    "    \"\"\"\n",
    "    Stop the training if there is no non-trivial progress in k steps\n",
    "    @param val_acc_history: a list contains all the historical validation acc\n",
    "    @param required_progress: the next acc should be higher than the previous by \n",
    "        at least required_progress amount to be non-trivial\n",
    "    @param t: number of training steps \n",
    "    @return: a boolean indicates if the model should earily stop\n",
    "    \"\"\"\n",
    "    # TODO: add your code here\n",
    "    \n",
    "    cnt = 0 # initialize the count --> to store count of cases where difference in\n",
    "                                    #  accuracy is less than required progress.\n",
    "    \n",
    "    if(len(val_acc_history) > 0): # if list has size > 0 \n",
    "        for i in range(t): # start the loop\n",
    "            index = len(val_acc_history) - (i+1) # start from the last term in list and move to the left\n",
    "            if (index >= 1): # to check if index != 0 --> else we can't compare to previous value\n",
    "                if (abs(val_acc_history[index] - val_acc_history[index-1]) < required_progress):\n",
    "                    cnt += 1 # increase the count value\n",
    "                else:\n",
    "                    break # break if difference is grea-ter \n",
    "    \n",
    "    if(cnt != t): # if count is equal to t, return True\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def train(valid_sequences, valid_label, num_epochs, optimizer, data_iter, model, training_length, threshold):\n",
    "    losses = []\n",
    "    total_batches = int(training_length/ batch_size) #375\n",
    "    \n",
    "    # Storing f-score, precision, recall, accuracy for every threshold.\n",
    "    #validation_acc_history = [[] for i in range(len(threshold))]\n",
    "    validation_losses = []\n",
    "    F_scores = []\n",
    "    calculated_f_score = np.zeros(len(threshold))\n",
    "    max_precision = np.zeros(len(threshold))\n",
    "    max_recall = np.zeros(len(threshold))\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        stop_training = False\n",
    "        for i, (train_data, train_labels, length_batch) in enumerate(data_iter):\n",
    "                                                # train_data size: (26, 34350) ; train_label size: (26, 147)\n",
    "                                                # This needs to be modified. Max length is batch specific !!!!!\n",
    "            model.train(True)\n",
    "            model.zero_grad()\n",
    "            outputs = model(train_data.transpose(0,1))\n",
    "            loss = criterion(outputs, train_labels.float())\n",
    "            losses.append(loss.data[0])\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            clipped = torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "            # clip gradients because RNN\n",
    "            for pr in model.parameters():\n",
    "                pr.data.add_(-clipped, pr.grad.data)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if i%eval_every == 0:\n",
    "#                 # Erly stop using validation loss\n",
    "                model.eval()\n",
    "                val_outputs = model(Variable((valid_sequences).transpose(0,1).type(torch.LongTensor), volatile=True))\n",
    "                eval_loss = criterion(val_outputs.data, valid_label.type(torch.FloatTensor))\n",
    "                print(eval_loss.data[0])\n",
    "                validation_losses.append(eval_loss.data[0])\n",
    "                stop_training = early_stop(validation_losses, 3)\n",
    "                \n",
    "#                 # Early stop using F-score (Yet to be done)\n",
    "#                 ave_precision = average_precision(val_outputs, Variable(valid_label), th)\n",
    "#                 ave_recall = average_recall(val_outputs, Variable(valid_label), th)\n",
    "#                 if ave_precision == 0 or ave_recall == 0:\n",
    "#                     f_score = 0\n",
    "#                 else:\n",
    "#                     f_score = F_score(ave_precision, ave_recall)\n",
    "#                 print(f_score)\n",
    "#                 F_scores.append(f_score)\n",
    "#                 stop_training = early_stop(F_scores, 3)\n",
    "                \n",
    "            \n",
    "            # Print statements\n",
    "            if stop_training:\n",
    "                print(\"earily stop triggered\")\n",
    "                break\n",
    "            if (i+1) % print_every == 0:\n",
    "#                 print('Epoch: [{0}/{1}], Step: [{2}/{3}], Train loss: {4}, F_Score: {5}, Validation loss:{6}'.format( \n",
    "#                            epoch, num_epochs, i+1, total_batches, np.mean(losses)/(total_batches*epoch), np.max(calculated_f_score), np.mean(np.array(validation_acc_history)[:,-1])))\n",
    "                print('Epoch: [{0}/{1}], Step: [{2}/{3}], Train loss: {4}, Validation loss:{5}'.format( \n",
    "                           epoch, num_epochs, i+1, total_batches, np.mean(losses)/(total_batches*epoch), np.mean(np.array(validation_losses))))\n",
    "        if stop_training == True:\n",
    "            break\n",
    "            \n",
    "    #return calculated_f_score, max_precision, max_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_size = len(train_seqsHuman) #9751\n",
    "num_labels = go_termsHuman.shape[0] #147\n",
    "hidden_size = 50\n",
    "eval_every = 2\n",
    "print_every = 5\n",
    "\n",
    "model = RNN_GRU(vocab_size, 50, num_labels, hidden_size, n_layers=1, dropout=0.1, is_bidirectional=False)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9509052038192749\n",
      "0.9491850733757019\n",
      "0.9474327564239502\n",
      "Epoch: [1/1], Step: [5/375], Train loss: 0.0025489463806152345, Validation loss:0.9491743445396423\n",
      "0.9455481767654419\n",
      "0.9434755444526672\n",
      "Epoch: [1/1], Step: [10/375], Train loss: 0.002546151399612427, Validation loss:0.9473093509674072\n",
      "0.9411472082138062\n",
      "0.9384976029396057\n",
      "0.9353588819503784\n",
      "Epoch: [1/1], Step: [15/375], Train loss: 0.0025402334637112086, Validation loss:0.9439438059926033\n",
      "0.9315640926361084\n",
      "0.9269000291824341\n",
      "Epoch: [1/1], Step: [20/375], Train loss: 0.002531848732630412, Validation loss:0.9410014569759368\n",
      "0.920922577381134\n",
      "0.9130644202232361\n",
      "0.9023003578186035\n",
      "Epoch: [1/1], Step: [25/375], Train loss: 0.002517262961069743, Validation loss:0.9343309173217187\n",
      "0.88697350025177\n",
      "0.8652839064598083\n",
      "Epoch: [1/1], Step: [30/375], Train loss: 0.0024940485583411325, Validation loss:0.9265706221262614\n",
      "0.8392112851142883\n",
      "0.8150308728218079\n",
      "0.7952311038970947\n",
      "Epoch: [1/1], Step: [35/375], Train loss: 0.0024549544243585497, Validation loss:0.9082240329848396\n",
      "0.7791734337806702\n",
      "0.766151487827301\n",
      "Epoch: [1/1], Step: [40/375], Train loss: 0.0024092511971791587, Validation loss:0.8946678757667541\n",
      "0.7555112838745117\n",
      "0.7467884421348572\n",
      "0.739589512348175\n",
      "Epoch: [1/1], Step: [45/375], Train loss: 0.002364807524504485, Validation loss:0.8754455110301143\n",
      "0.733676552772522\n",
      "0.7287337779998779\n",
      "Epoch: [1/1], Step: [50/375], Train loss: 0.0023244932174682617, Validation loss:0.8639062833786011\n",
      "0.7245996594429016\n",
      "0.7211375832557678\n",
      "0.7182130813598633\n",
      "Epoch: [1/1], Step: [55/375], Train loss: 0.0022888025226015032, Validation loss:0.8486288360186985\n",
      "0.7157281637191772\n",
      "0.7135985493659973\n",
      "Epoch: [1/1], Step: [60/375], Train loss: 0.0022574839803907606, Validation loss:0.8396978040536245\n",
      "0.711772084236145\n",
      "0.7102209329605103\n",
      "0.7088603377342224\n",
      "Epoch: [1/1], Step: [65/375], Train loss: 0.0022299156286777594, Validation loss:0.8279329538345337\n",
      "0.7076870203018188\n",
      "0.706657886505127\n",
      "Epoch: [1/1], Step: [70/375], Train loss: 0.002205614155814761, Validation loss:0.8210323538099017\n",
      "0.7057642340660095\n",
      "0.7049700617790222\n",
      "0.7042616009712219\n",
      "Epoch: [1/1], Step: [75/375], Train loss: 0.0021841557545132107, Validation loss:0.8118717968463898\n",
      "0.7036479115486145\n",
      "0.7030807137489319\n",
      "Epoch: [1/1], Step: [80/375], Train loss: 0.002165028045574824, Validation loss:0.8064464226365089\n",
      "0.7025740146636963\n",
      "0.7021161913871765\n",
      "0.7017101645469666\n",
      "Epoch: [1/1], Step: [85/375], Train loss: 0.0021479662072424795, Validation loss:0.7991687738618185\n",
      "0.7013366222381592\n",
      "0.7009927034378052\n",
      "Epoch: [1/1], Step: [90/375], Train loss: 0.0021326287817071987, Validation loss:0.7948130355940924\n",
      "0.7006711959838867\n",
      "0.7003819346427917\n",
      "0.7001157999038696\n",
      "Epoch: [1/1], Step: [95/375], Train loss: 0.002118773211094371, Validation loss:0.7889115735888481\n",
      "0.6998721957206726\n",
      "0.6996511220932007\n",
      "Epoch: [1/1], Step: [100/375], Train loss: 0.0021062191279729208, Validation loss:0.7853455770015717\n",
      "0.6994385719299316\n",
      "0.6992394924163818\n",
      "0.6990535259246826\n",
      "Epoch: [1/1], Step: [105/375], Train loss: 0.002094784747229682, Validation loss:0.7804718951009354\n",
      "0.6988713145256042\n",
      "0.6986937522888184\n",
      "Epoch: [1/1], Step: [110/375], Train loss: 0.0020843411402268843, Validation loss:0.7775013728575273\n",
      "0.6985421776771545\n",
      "0.6984031796455383\n",
      "0.6982527375221252\n",
      "Epoch: [1/1], Step: [115/375], Train loss: 0.002074745143669239, Validation loss:0.7734098896898073\n",
      "0.6981198787689209\n",
      "0.6979930400848389\n",
      "Epoch: [1/1], Step: [120/375], Train loss: 0.0020659129367934332, Validation loss:0.770898108681043\n",
      "0.6978695392608643\n",
      "0.6977537274360657\n",
      "0.6976534128189087\n",
      "Epoch: [1/1], Step: [125/375], Train loss: 0.0020577570215861004, Validation loss:0.7674152888948955\n",
      "0.6975422501564026\n",
      "0.6974403858184814\n",
      "Epoch: [1/1], Step: [130/375], Train loss: 0.002050195225691184, Validation loss:0.7652637820977432\n",
      "0.6973415613174438\n",
      "0.6972483396530151\n",
      "0.6971617341041565\n",
      "Epoch: [1/1], Step: [135/375], Train loss: 0.0020431596367447465, Validation loss:0.762263198109234\n",
      "0.6970748901367188\n",
      "0.6969836354255676\n",
      "Epoch: [1/1], Step: [140/375], Train loss: 0.0020366117329824536, Validation loss:0.7603993713855743\n",
      "0.6969037055969238\n",
      "0.6968204379081726\n",
      "0.6967502236366272\n",
      "Epoch: [1/1], Step: [145/375], Train loss: 0.002030480556926508, Validation loss:0.7577867173168757\n",
      "0.6966772079467773\n",
      "0.696614146232605\n",
      "Epoch: [1/1], Step: [150/375], Train loss: 0.0020247460047403973, Validation loss:0.7561562895774842\n",
      "0.6965481638908386\n",
      "0.6964839696884155\n",
      "0.6964206695556641\n",
      "Epoch: [1/1], Step: [155/375], Train loss: 0.002019374442356889, Validation loss:0.7538612118134131\n",
      "0.6963688731193542\n",
      "0.696313738822937\n",
      "Epoch: [1/1], Step: [160/375], Train loss: 0.002014323049783707, Validation loss:0.7524232141673565\n",
      "0.6962617039680481\n",
      "0.6962023973464966\n",
      "0.6961439251899719\n",
      "Epoch: [1/1], Step: [165/375], Train loss: 0.002009566033488572, Validation loss:0.7503911465047354\n",
      "0.6960831880569458\n",
      "0.6960267424583435\n",
      "Epoch: [1/1], Step: [170/375], Train loss: 0.0020050753490597597, Validation loss:0.7491126481224509\n",
      "0.6959875226020813\n",
      "0.6959334015846252\n",
      "0.6958930492401123\n",
      "Epoch: [1/1], Step: [175/375], Train loss: 0.002000837165287563, Validation loss:0.7472998757253994\n",
      "0.6958414912223816\n",
      "0.6957951188087463\n",
      "Epoch: [1/1], Step: [180/375], Train loss: 0.001996822483451278, Validation loss:0.7461558408207364\n",
      "0.6957530975341797\n",
      "0.6957183480262756\n",
      "0.6956844329833984\n",
      "Epoch: [1/1], Step: [185/375], Train loss: 0.001993015514408146, Validation loss:0.7445288338968831\n",
      "0.6956400275230408\n",
      "0.6956025958061218\n",
      "Epoch: [1/1], Step: [190/375], Train loss: 0.0019894091982590525, Validation loss:0.7434992018498873\n",
      "0.6955668330192566\n",
      "0.6955240964889526\n",
      "0.6954904794692993\n",
      "Epoch: [1/1], Step: [195/375], Train loss: 0.001985965990408873, Validation loss:0.742030669231804\n",
      "0.6954492926597595\n",
      "0.6954225897789001\n",
      "Epoch: [1/1], Step: [200/375], Train loss: 0.001982683874766032, Validation loss:0.7410987746715546\n",
      "0.6953880190849304\n",
      "0.6953627467155457\n",
      "0.6953333616256714\n",
      "Epoch: [1/1], Step: [205/375], Train loss: 0.001979568948978331, Validation loss:0.7397666174231223\n",
      "0.6953067779541016\n",
      "0.6952801942825317\n",
      "Epoch: [1/1], Step: [210/375], Train loss: 0.0019765878162686787, Validation loss:0.7389195101601737\n",
      "0.6952403783798218\n",
      "0.6952033638954163\n",
      "0.6951748132705688\n",
      "Epoch: [1/1], Step: [215/375], Train loss: 0.001973749910398971, Validation loss:0.7377052511330005\n",
      "0.6951480507850647\n",
      "0.6951229572296143\n",
      "Epoch: [1/1], Step: [220/375], Train loss: 0.0019710384209950765, Validation loss:0.7369312557307157\n",
      "0.6950957775115967\n",
      "0.6950725317001343\n",
      "0.6950511932373047\n",
      "Epoch: [1/1], Step: [225/375], Train loss: 0.001968433893698233, Validation loss:0.735819979051573\n",
      "0.6950283050537109\n",
      "0.6950042247772217\n",
      "Epoch: [1/1], Step: [230/375], Train loss: 0.0019659455700197083, Validation loss:0.7351103492405103\n",
      "0.6949729323387146\n",
      "0.6949519515037537\n",
      "0.6949288249015808\n",
      "Epoch: [1/1], Step: [235/375], Train loss: 0.001963553989356291, Validation loss:0.7340893548423961\n",
      "0.6949084401130676\n",
      "0.6948884129524231\n",
      "Epoch: [1/1], Step: [240/375], Train loss: 0.0019612569676505194, Validation loss:0.7334361727039019\n",
      "0.6948695778846741\n",
      "0.694850742816925\n",
      "0.6948303580284119\n",
      "Epoch: [1/1], Step: [245/375], Train loss: 0.001959055154501986, Validation loss:0.7324950520585223\n",
      "0.6948039531707764\n",
      "0.6947814226150513\n",
      "Epoch: [1/1], Step: [250/375], Train loss: 0.00195694109026591, Validation loss:0.7318918142318726\n",
      "0.6947622299194336\n",
      "0.6947502493858337\n",
      "0.6947321891784668\n",
      "Epoch: [1/1], Step: [255/375], Train loss: 0.001954906651864644, Validation loss:0.7310212613083422\n",
      "0.6947117447853088\n",
      "0.6946890354156494\n",
      "Epoch: [1/1], Step: [260/375], Train loss: 0.0019529495948400252, Validation loss:0.7304624786743751\n",
      "0.694672703742981\n",
      "0.6946561932563782\n",
      "0.694641649723053\n",
      "Epoch: [1/1], Step: [265/375], Train loss: 0.0019510592076763418, Validation loss:0.7296548328901592\n",
      "0.694624662399292\n",
      "0.6946032047271729\n",
      "Epoch: [1/1], Step: [270/375], Train loss: 0.0019492359167263833, Validation loss:0.7291357084556862\n",
      "0.694585382938385\n",
      "0.6945711970329285\n",
      "0.6945584416389465\n",
      "Epoch: [1/1], Step: [275/375], Train loss: 0.0019474769534486714, Validation loss:0.7283843163994775\n",
      "0.6945459842681885\n",
      "0.6945369839668274\n",
      "Epoch: [1/1], Step: [280/375], Train loss: 0.0019457796590668815, Validation loss:0.727900847366878\n",
      "0.6945216059684753\n",
      "0.6945081353187561\n",
      "0.6944965720176697\n",
      "Epoch: [1/1], Step: [285/375], Train loss: 0.0019441410282201935, Validation loss:0.7272003142983763\n",
      "0.6944741010665894\n"
     ]
    }
   ],
   "source": [
    "data_iter = batch_iter(batch_size, TrainSeqsHuman, yTrainHuman, train_seqsHuman_length)\n",
    "\n",
    "num_epochs = 1\n",
    "threshold = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 1]\n",
    "\n",
    "# Model Training\n",
    "ValidSeqsHuman_small, yValidHuman_small = reduced_set(ValidSeqsHuman, valid_seqsHuman_length, yValidHuman, 100)\n",
    "#f2, p2, r2 = train(ValidSeqsHuman_small, yValidHuman_small, num_epochs, optimizer, data_iter, model, data_size, threshold)\n",
    "train(ValidSeqsHuman_small, yValidHuman_small, num_epochs, optimizer, data_iter, model, data_size, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Testing performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_set_predictions(model, test_input_seq):\n",
    "    model.eval()\n",
    "    test_input_seq = Variable(test_input_seq)\n",
    "    predicted = model(test_input_seq.transpose(0,1).type(torch.LongTensor))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predictions = test_set_predictions(model, TestSeqsHuman)\n",
    "#FScore, Threshold, Precision, Recall = F_score(test_set_predictions, yTestHuman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_AUPR_curve(test_predictions.data.numpy(), yTestHuman.numpy(), label='GRU', org='Human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_AUC_curve(test_predictions.data.numpy(), yTestHuman.numpy(), label='GRU', org='Human')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
