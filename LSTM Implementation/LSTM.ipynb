{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from Bio import SeqIO\n",
    "import collections\n",
    "import sys\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"..\")\n",
    "import data_processing as dp\n",
    "import evaluation_metrics as em\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Create Train/Dev/Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human_sequences, human_protein_names = dp.load_FASTA('../data/human_sequences.fasta')\n",
    "human_train_idx, human_valid_idx, human_test_idx, human_train_labels, human_valid_labels, \\\n",
    "    human_test_labels, human_GO_terms = dp.load_test_sets('../data/human_annotations_temporal_holdout.mat')\n",
    "\n",
    "# Create train, validation, and test sets from the full list of human proteins\n",
    "human_train_sequences = [human_sequences[i] for i in human_train_idx]\n",
    "human_valid_sequences = [human_sequences[i] for i in human_valid_idx]\n",
    "human_test_sequences = [human_sequences[i] for i in human_test_idx]\n",
    "\n",
    "# Convert corresponding labels for train, validation, and test sets \n",
    "# from the full list of human proteins.\n",
    "human_train_labels = torch.from_numpy(human_train_labels).type(torch.LongTensor)\n",
    "human_valid_labels = torch.from_numpy(human_valid_labels).type(torch.LongTensor)\n",
    "human_test_labels = torch.from_numpy(human_test_labels).type(torch.LongTensor)\n",
    "\n",
    "# Convert protein sequence strings into long tensors where each int corresponds\n",
    "# to one of 22 amino acids.  The length to truncate to is included.\n",
    "human_train_tensors = dp.TransformAAsToTensor(human_train_sequences,1000)\n",
    "human_valid_tensors = dp.TransformAAsToTensor(human_valid_sequences,1000)\n",
    "human_test_tensors = dp.TransformAAsToTensor(human_test_sequences,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yeast sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load yeast sequences and training data\n",
    "yeast_sequences, yeast_protein_names = dp.load_FASTA('../data/yeast_sequences.fasta')\n",
    "yeast_train_idx, yeast_valid_idx, yeast_test_idx, yeast_train_labels, yeast_valid_labels, \\\n",
    "    yeast_test_labels, yeast_GO_terms = dp.load_test_sets('../data/yeast_MF_temporal_holdout.mat')\n",
    "\n",
    "# Create train, validation, and test sets from the full list of yeast proteins\n",
    "yeast_train_sequences = [yeast_sequences[i] for i in yeast_train_idx]\n",
    "yeast_valid_sequences = [yeast_sequences[i] for i in yeast_valid_idx]\n",
    "yeast_test_sequences = [yeast_sequences[i] for i in yeast_test_idx]\n",
    "\n",
    "# Convert corresponding labels for train, validation, and test sets from the full list of yeast proteins.\n",
    "yeast_train_labels = torch.from_numpy(yeast_train_labels).type(torch.LongTensor)\n",
    "yeast_valid_labels = torch.from_numpy(yeast_valid_labels).type(torch.LongTensor)\n",
    "yeast_test_labels = torch.from_numpy(yeast_test_labels).type(torch.LongTensor)\n",
    "\n",
    "# Convert protein sequence strings into long tensors where each int corresponds\n",
    "# to one of 22 amino acids.  The length to truncate to is included.\n",
    "yeast_train_tensors = dp.TransformAAsToTensor(yeast_train_sequences,500)\n",
    "yeast_valid_tensors = dp.TransformAAsToTensor(yeast_valid_sequences,500)\n",
    "yeast_test_tensors = dp.TransformAAsToTensor(yeast_test_sequences,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM class:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_labels, batch_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)        \n",
    "        self.embedding_size = emb_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = num_labels\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.linear_f = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.linear_i = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.linear_ctilde = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.linear_o = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, data, hidden, c):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        emb = self.embed(data)\n",
    "        embs = torch.chunk(emb, emb.size()[1], 1)\n",
    "        \n",
    "        def step(emb, hid, c_t):\n",
    "            combined = torch.cat((hid,emb),1)\n",
    "            f = F.sigmoid(self.linear_f(combined))\n",
    "            i = F.sigmoid(self.linear_i(combined))\n",
    "            c_tilde = F.tanh(self.linear_ctilde(combined))\n",
    "            c_t = f*c_t + i*c_tilde\n",
    "            o = F.sigmoid(self.linear_o(combined))\n",
    "            hid = o * F.tanh(c_t)\n",
    "            return hid, c_t\n",
    "        \n",
    "        for i in range(len(embs)):\n",
    "            hidden, c = step(embs[i].squeeze(), hidden, c)\n",
    "        \n",
    "        output = self.decoder(hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(self.batch_size, self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.batch_size, self.hidden_size))\n",
    "        return h0, c0\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_f, self.linear_i, self.linear_ctilde, self.linear_o]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stop condition and training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def early_stop(val_loss_history, t=10, required_progress=0.001):\n",
    "    \"\"\"\n",
    "    Stop the training if there is no non-trivial progress in k steps\n",
    "    @param val_acc_history: a list contains all the historical validation acc\n",
    "    @param required_progress: the next acc should be higher than the previous by \n",
    "        at least required_progress amount to be non-trivial\n",
    "    @param t: number of training steps \n",
    "    @return: a boolean indicates if the model should earily stop\n",
    "    \"\"\"    \n",
    "    cnt = 0 # initialize the count --> to store count of cases where difference in\n",
    "                                    #  accuracy is less than required progress.\n",
    "    \n",
    "    if(len(val_loss_history) > 0): # if list has size > 0 \n",
    "        for i in range(t): # start the loop\n",
    "            index = len(val_loss_history) - (i+1) # start from the last term in list and move to the left\n",
    "            if (index >= 1): # to check if index != 0 --> else we can't compare to previous value\n",
    "                if ((val_loss_history[index-1] - val_loss_history[index]) < required_progress):\n",
    "                    cnt += 1 # increase the count value\n",
    "                else:\n",
    "                    break # break if difference is greater \n",
    "    \n",
    "    if(cnt != t): # if count is equal to t, return True\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,average_precision_score\n",
    "\n",
    "def round_manual(data, threshold):\n",
    "    return (data >= threshold).astype(int)\n",
    "\n",
    "def calculate_accuracy(predicted, actuals, num_labels):\n",
    "    \"\"\"\n",
    "    @param predicted: data type = Variable\n",
    "    @param actuals: data type = Variable\n",
    "    @param num_labels: no of go terms\n",
    "    @return: accuracy measure\n",
    "    \"\"\"\n",
    "    predicted = np.round(predicted.data.numpy())\n",
    "    total_predictions = actuals.size()[0]\n",
    "    accuracy = np.sum(predicted==actuals.data.numpy())/(total_predictions*num_labels)\n",
    "    return accuracy\n",
    "\n",
    "def m_tau(predictions):\n",
    "    return len([np.sum(i) for i in predictions if np.sum(i)!=0])\n",
    "\n",
    "def n_e(predictions):\n",
    "    return predictions.shape[0]\n",
    "\n",
    "def calculate_recall_precision(predicted, actual):\n",
    "    '''\n",
    "    Overall, this function calculates the recall and precision of the validation set proteins.\n",
    "    The function FIRST calculates the precision and recall values of INDIVIDUAL proteins. \n",
    "    It then takes the mean average of these values to get \"dataset-level\" precision and recall.\n",
    "    '''\n",
    "    \n",
    "    PositivesPerRow = actual.numpy().sum(axis=1) #number of functions for each protein\n",
    "    PosPredictionsPerRow = predicted.sum(axis=1) #number of predictions for each protein\n",
    "    TPs = np.multiply(actual.numpy(), predicted) #element-wise multiplication: 1 if TP, else 0\n",
    "    TPsPerRow = TPs.sum(axis=1) #number of true positives for each protein\n",
    "    \n",
    "    #PrecisionPerRow (Protein) - if protein has 0 positive predictions, the protein's precision = 0.\n",
    "    #Else, the protein's precision = TPs/PositivePreds\n",
    "    PrecisionPerRow = np.where(PosPredictionsPerRow == 0, 0, TPsPerRow/PosPredictionsPerRow)\n",
    "    RecallPerRow = np.where(PositivesPerRow==0, 0, TPsPerRow/PositivesPerRow) #Recall per Protein\n",
    "    \n",
    "    #RecallScore = average of individual protein recall scores\n",
    "    RecallScore = sum(RecallPerRow)/len(RecallPerRow) #denominator is non-zero\n",
    "    \n",
    "    #PrecisionScore = average of CERTAIN individual protein precision scores (see line below)\n",
    "    #Only consider rows with at least one predicted Go-Term.\n",
    "    #Note that some proteins can have Precision=0 but still have predictions.\n",
    "    if sum(PrecisionPerRow)>0:\n",
    "        PrecisionScore = sum(PrecisionPerRow)/len([x for x in PosPredictionsPerRow if x!=0]) \n",
    "    else:\n",
    "        PrecisionScore = 0\n",
    "    return RecallScore, PrecisionScore\n",
    "    \n",
    "    \n",
    "def F_score(predicted, actuals):\n",
    "    \"\"\"\n",
    "    @param predicted: data type = Variable\n",
    "    @param actuals: data type = Variable\n",
    "    @return: Maximum f score over all values of tau and the corresponding tau threshold\n",
    "    \"\"\"\n",
    "    f_max, optimal_threshold, optimal_precision, optimal_recall = 0, 0, 0, 0\n",
    "    for threshold in [i/100 for i in range(1,100)]:\n",
    "        predicted_tau = round_manual(predicted.data.numpy(), threshold)\n",
    "        recall_score, precision_score = calculate_recall_precision(predicted_tau, actuals)\n",
    "        \n",
    "        if recall_score==0 and precision_score==0:\n",
    "            output = 0\n",
    "        else:\n",
    "            output = np.true_divide((2*precision_score*recall_score),(precision_score + recall_score))\n",
    "        if output > f_max:\n",
    "            f_max = output\n",
    "            optimal_threshold = threshold\n",
    "            optimal_precision = precision_score\n",
    "            optimal_recall = recall_score\n",
    "    \n",
    "    return f_max, optimal_threshold, optimal_precision, optimal_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(batch_size,num_epochs,model,loss,optimizer,train_batch,test_eval_batch,\\\n",
    "               num_labels,lstm=True, early_stop=False):\n",
    "    eval_step=0\n",
    "    train_step=0\n",
    "    epoch=1\n",
    "    losses = []\n",
    "    valid_loss_history = []\n",
    "    if num_labels == 147:\n",
    "        total_batches = int(len(human_train_tensors)/batch_size)\n",
    "    else:\n",
    "        total_batches = int(len(yeast_train_tensors)/batch_size)\n",
    "        \n",
    "    while epoch < num_epochs:\n",
    "        train_data, train_labels = next(data_iter)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if lstm:\n",
    "            hidden, c_t = model.init_hidden()\n",
    "            outputs, hidden = model(train_data, hidden, c_t)\n",
    "        else:\n",
    "            outputs = model(train_data, length_batch)\n",
    "\n",
    "        loss = criterion(outputs, train_labels.float())\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()          \n",
    "        \n",
    "        if early_stop:\n",
    "            stop_training = early_stop(valid_loss_history)\n",
    "\n",
    "            if stop_training:\n",
    "                print(\"earily stop triggered\")\n",
    "                break\n",
    "        \n",
    "        if train_step % 10 == 0:\n",
    "            model.eval()\n",
    "            if lstm:\n",
    "                hidden, c_t = model.init_hidden()\n",
    "                valid_outputs, hidden = model(Variable(test_eval_batch[eval_step][0]), hidden, c_t)                \n",
    "            else:\n",
    "                valid_outputs = model(Variable(valid_sequences))\n",
    "\n",
    "            f_score,_,_,_ = F_score(valid_outputs, test_eval_batch[eval_step][1])\n",
    "\n",
    "            valid_loss = criterion(valid_outputs.data, test_eval_batch[eval_step][1].float()).data.numpy()\n",
    "            valid_loss_history.append(valid_loss)\n",
    "\n",
    "            print('Epoch: [{}/{}], Train loss: {}, Validation Loss:{},Valid F_Score: {}'\\\n",
    "                  .format(epoch, num_epochs, np.mean(losses),valid_loss_history[-1], f_score))\n",
    "            eval_step+=1\n",
    "            \n",
    "        if train_step % total_batches==0:\n",
    "            torch.save(model.state_dict(), PATH) # Saves model after every epoch\n",
    "            epoch+=1\n",
    "        \n",
    "        train_step+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "vocab_size = 23 # number words in the vocabulary base\n",
    "emb_dim = 8 # dimension for n-gram embedding\n",
    "hidden_dim=12\n",
    "num_epochs = 50 # number epoch to train\n",
    "batch_size = 100\n",
    "PATH='saved_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = human_GO_terms.shape[0] #147\n",
    "\n",
    "lstm = LSTM(vocab_size, emb_dim, hidden_dim,num_labels,batch_size)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()  \n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) \n",
    "\n",
    "data_iter = dp.batch_iter(batch_size, human_train_tensors, human_train_labels)\n",
    "valid_eval = dp.eval_iter(batch_size, human_valid_tensors, human_valid_labels)\n",
    "\n",
    "# Model Training\n",
    "train_test(batch_size, num_epochs, lstm, criterion, optimizer, data_iter, valid_eval,num_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yeast Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = yeast_GO_terms.shape[0] #147\n",
    "\n",
    "lstm = LSTM(vocab_size, emb_dim, hidden_dim,num_labels,batch_size)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "\n",
    "data_iter = batch_iter(batch_size, yeast_train_tensors, yeast_train_labels)\n",
    "valid_eval = dp.eval_iter(batch_size, yeast_valid_tensors, yeast_valid_labels)\n",
    "\n",
    "# Model Training\n",
    "train_test(batch_size, num_epochs, lstm, criterion, optimizer, data_iter, valid_eval,num_labels) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
