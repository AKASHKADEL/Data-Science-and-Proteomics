{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from Bio import SeqIO\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def load_test_sets(filename):\n",
    "    go_data = sio.loadmat(filename, squeeze_me=True)\n",
    "    go_terms = go_data['goTerm_labels'] # names of gene ontology function terms\n",
    "    train_annotations = np.asarray(go_data['trainProts_label'].todense()) # training set of function annotations\n",
    "    valid_annotations = np.asarray(go_data['validProts_label'].todense()) # valid \"\" \"\"\n",
    "    test_annotations = np.asarray(go_data['testProts_label'].todense()) # test \"\" \"\"\n",
    "    train_inds = go_data['trainProts']\n",
    "    train_inds = train_inds - 1\n",
    "    valid_inds = go_data['validProts']\n",
    "    valid_inds = valid_inds - 1\n",
    "    test_inds = go_data['testProts']\n",
    "    test_inds = test_inds - 1 # subtract 1 for matlab index conversion into python\n",
    "\n",
    "    return train_inds, valid_inds, test_inds, train_annotations, valid_annotations, test_annotations, go_terms\n",
    "\n",
    "def load_FASTA(filename):\n",
    "    \"\"\" Loads fasta file and returns a list of the Bio SeqIO records \"\"\"\n",
    "    infile = open(filename)\n",
    "    full_entries = list(SeqIO.parse(infile, 'fasta'))\n",
    "    sequences = [str(entry.seq) for entry in full_entries]\n",
    "    names = [str(entry.id) for entry in full_entries]\n",
    "\n",
    "    return sequences, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load human sequences and create train/dev/test sets, create lengths for FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "human_sequences, human_protein_names = load_FASTA('../data/human_sequences.fasta')\n",
    "human_train_idx, human_valid_idx, human_test_idx, human_train_labels, human_valid_labels, \\\n",
    "    human_test_labels, human_GO_terms = load_test_sets('../data/human_annotations_temporal_holdout.mat')\n",
    "\n",
    "# Create train, validation, and test sets from the full list of human proteins\n",
    "human_train_sequences = [human_sequences[i] for i in human_train_idx]\n",
    "human_valid_sequences = [human_sequences[i] for i in human_valid_idx]\n",
    "human_test_sequences = [human_sequences[i] for i in human_test_idx]\n",
    "\n",
    "# Convert corresponding labels for train, validation, and test sets from the full list of human proteins.\n",
    "human_train_labels = torch.from_numpy(human_train_labels).type(torch.LongTensor)\n",
    "human_valid_labels = torch.from_numpy(human_valid_labels).type(torch.LongTensor)\n",
    "human_test_labels = torch.from_numpy(human_test_labels).type(torch.LongTensor)\n",
    "\n",
    "# Create lengths for sequence representation averaging in FastText\n",
    "human_train_lengths = torch.LongTensor([len(human_train_sequences[i]) for i in range(len(human_train_sequences))])\n",
    "human_valid_lengths = torch.LongTensor([len(human_valid_sequences[i]) for i in range(len(human_valid_sequences))])\n",
    "human_test_lengths = torch.LongTensor([len(human_test_sequences[i]) for i in range(len(human_test_sequences))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load yeast sequences and create train/dev/test sets, create lengths for FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load yeast sequences and training data\n",
    "yeast_sequences, yeast_protein_names = load_FASTA('../data/yeast_sequences.fasta')\n",
    "yeast_train_idx, yeast_valid_idx, yeast_test_idx, yeast_train_labels, yeast_valid_labels, \\\n",
    "    yeast_test_labels, yeast_GO_terms = load_test_sets('../data/yeast_MF_temporal_holdout.mat')\n",
    "\n",
    "# Create train, validation, and test sets from the full list of yeast proteins\n",
    "yeast_train_sequences = [yeast_sequences[i] for i in yeast_train_idx]\n",
    "yeast_valid_sequences = [yeast_sequences[i] for i in yeast_valid_idx]\n",
    "yeast_test_sequences = [yeast_sequences[i] for i in yeast_test_idx]\n",
    "\n",
    "# Convert corresponding labels for train, validation, and test sets from the full list of yeast proteins.\n",
    "yeast_train_labels = torch.from_numpy(yeast_train_labels).type(torch.LongTensor)\n",
    "yeast_valid_labels = torch.from_numpy(yeast_valid_labels).type(torch.LongTensor)\n",
    "yeast_test_labels = torch.from_numpy(yeast_test_labels).type(torch.LongTensor)\n",
    "\n",
    "# Create lengths for sequence representation averaging in FastText\n",
    "yeast_train_lengths = torch.LongTensor([len(yeast_train_sequences[i]) for i in range(len(yeast_train_sequences))])\n",
    "yeast_valid_lengths = torch.LongTensor([len(yeast_valid_sequences[i]) for i in range(len(yeast_valid_sequences))])\n",
    "yeast_test_lengths = torch.LongTensor([len(yeast_test_sequences[i]) for i in range(len(yeast_test_sequences))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed amino-acid chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each amino-acid string becomes an NxD entry in a tensor, where N is the number of \n",
    "# amino-acid strings and D is the length of the longest chain in the set. \n",
    "\n",
    "ConvertCharToInt = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'H':8, 'I':9, 'J':10,\n",
    "                   'K':11, 'L':12, 'M':13, 'N':14, 'O':15, 'P':16, 'Q':17, 'R':18, 'S':19,\n",
    "                   'T':20, 'U':21, 'V':22, 'W':23, 'X':24, 'Y':25, 'Z':26}\n",
    "\n",
    "def vectorize_AAs(string):\n",
    "    '''This function takes an amino-acid string as input and outputs a vector of integers, with each\n",
    "    integer representing one amino acid.\n",
    "    \n",
    "    For example, 'BACEA' is converted to [2, 1, 3, 5, 1]\n",
    "    '''\n",
    "    character_list = list(string) #converts 'BACEA' to ['B','A','C','E','A]\n",
    "    for i in range(len(character_list)):\n",
    "        character_list[i] = ConvertCharToInt[character_list[i]] #convert the character to a number\n",
    "    return character_list\n",
    "\n",
    "def AddZeros(vector, max_length):\n",
    "    '''This function adds the necessary number of zeros and returns an array'''\n",
    "    #max_length = length of longest vector in the batch\n",
    "    #oldvector = initial vector for that amino-acid chain (in integers)\n",
    "    diff = max_length - len(vector)\n",
    "    if diff>0:\n",
    "        ZerosToAdd = np.zeros(diff)\n",
    "        vector.extend(ZerosToAdd)\n",
    "    return vector \n",
    "\n",
    "def TransformAAsToTensor(ListOfSequences):\n",
    "    '''This function takes as input a list of amino acid strings and creates a tensor matrix\n",
    "    of dimension NxD, where N is the number of strings and D is the length of the longest AA chain\n",
    "    \n",
    "    \"ListOfSequences\" can be training, validation, or test sets\n",
    "    '''\n",
    "    #find longest amino-acid sequence\n",
    "    max_length = len(max(ListOfSequences, key=len))\n",
    "    Sequences = ListOfSequences.copy() \n",
    "    for AA in range(len(Sequences)): #for each amino-acid sequence\n",
    "        Sequences[AA] = vectorize_AAs(Sequences[AA])\n",
    "        Sequences[AA] = AddZeros(Sequences[AA], max_length)\n",
    "    NewTensor = torch.from_numpy(np.array(Sequences))\n",
    "    return NewTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human_train_vectors = TransformAAsToTensor(human_train_sequences)\n",
    "human_valid_vectors = TransformAAsToTensor(human_valid_sequences)\n",
    "human_test_vectors = TransformAAsToTensor(human_test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yeast_train_vectors = TransformAAsToTensor(yeast_train_sequences)\n",
    "yeast_valid_vectors = TransformAAsToTensor(yeast_valid_sequences)\n",
    "yeast_test_vectors = TransformAAsToTensor(yeast_test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Hyperparameter setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Get batch data method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def batch_iter(TrainSeqs, yTrain, TrainSeqsLength, batch_size):\n",
    "    start = -1 * batch_size\n",
    "    dataset_size = TrainSeqs.size()[0]\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch_indices_tensor = torch.LongTensor(batch_indices)\n",
    "        batch_train = TrainSeqs[batch_indices_tensor].type(torch.LongTensor)\n",
    "        batch_train_labels = yTrain[batch_indices_tensor]\n",
    "        length_batch = TrainSeqsLength[batch_indices_tensor]\n",
    "        yield [Variable(batch_train), Variable(batch_train_labels), Variable(length_batch)]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) FastText class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    \"\"\"\n",
    "    FastText model\n",
    "    \"\"\"\n",
    "       \n",
    "    def __init__(self, vocab_size, emb_dim, num_labels):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(FastText, self).__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.embed = nn.Embedding(vocab_size+1, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,num_labels)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        data = data.type(torch.LongTensor)\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out = out / length.view(-1,1).float()\n",
    "            \n",
    "        out = self.linear(out)\n",
    "        return nn.functional.sigmoid(out)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,average_precision_score\n",
    "\n",
    "def round_manual(data, threshold):\n",
    "    return (data >= threshold).astype(int)\n",
    "\n",
    "def calculate_accuracy(predicted, actuals, num_labels):\n",
    "    \"\"\"\n",
    "    @param predicted: data type = Variable\n",
    "    @param actuals: data type = Variable\n",
    "    @param num_labels: no of go terms\n",
    "    @return: accuracy measure\n",
    "    \"\"\"\n",
    "    predicted = np.round(predicted.data.numpy())\n",
    "    total_predictions = actuals.size()[0]\n",
    "    accuracy = np.sum(predicted==actuals.data.numpy())/(total_predictions*num_labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def recall_precision_ProteinMethod(predicted, actual):\n",
    "    '''\n",
    "    Overall, this function calculates the recall and precision of the validation set proteins.\n",
    "    The function FIRST calculates the precision and recall values of INDIVIDUAL proteins. \n",
    "    It then takes the mean average of these values to get \"dataset-level\" precision and recall.\n",
    "    '''\n",
    "    \n",
    "    PositivesPerRow = actual.numpy().sum(axis=1) #number of functions for each protein\n",
    "    PosPredictionsPerRow = predicted.sum(axis=1) #number of predictions for each protein\n",
    "    TPs = np.multiply(actual.numpy(), predicted) #element-wise multiplication: 1 if TP, else 0\n",
    "    TPsPerRow = TPs.sum(axis=1) #number of true positives for each protein\n",
    "    \n",
    "    #PrecisionPerRow (Protein) - if protein has 0 positive predictions, the protein's precision = 0.\n",
    "    #Else, the protein's precision = TPs/PositivePreds\n",
    "    PrecisionPerRow = np.where(PosPredictionsPerRow == 0, 0, TPsPerRow/PosPredictionsPerRow)\n",
    "    RecallPerRow = np.where(PositivesPerRow==0, 0, TPsPerRow/PositivesPerRow) #Recall per Protein\n",
    "    \n",
    "    #RecallScore = average of individual protein recall scores\n",
    "    RecallScore = sum(RecallPerRow)/len(RecallPerRow) #denominator is non-zero\n",
    "    \n",
    "    #PrecisionScore = average of CERTAIN individual protein precision scores (see line below)\n",
    "    #Only consider rows with at least one predicted Go-Term.\n",
    "    #Note that some proteins can have Precision=0 but still have predictions.\n",
    "    if sum(PrecisionPerRow)>0:\n",
    "        PrecisionScore = sum(PrecisionPerRow)/len([x for x in PosPredictionsPerRow if x!=0]) \n",
    "    else:\n",
    "        PrecisionScore = 0\n",
    "    return RecallScore, PrecisionScore\n",
    "\n",
    "\n",
    "def recall_precision_GoTermMethod(predicted, actual):\n",
    "    '''\n",
    "    The function FIRST calculates the precision and recall values of INDIVIDUAL Go-Terms. \n",
    "    It then takes the mean average of these values to get \"dataset-level\" precision and recall.\n",
    "    '''\n",
    "    PositivesPerGoTerm = actual.numpy().sum(axis=0) #number of functions for each protein\n",
    "    PosPredictionsPerGoTerm = predicted.sum(axis=0) #number of predictions for each protein\n",
    "    TPs = np.multiply(actual.numpy(), predicted) #element-wise multiplication: 1 if TP, else 0\n",
    "    TPsPerGoTerm = TPs.sum(axis=0) #number of true positives for each protein\n",
    "    \n",
    "    PrecisionPerGoTerm = np.where(PosPredictionsPerGoTerm == 0, 0, TPsPerGoTerm/PosPredictionsPerGoTerm)\n",
    "    RecallPerGoTerm = np.where(PositivesPerGoTerm==0, 0, TPsPerGoTerm/PositivesPerGoTerm) #Recall per Protein\n",
    "    \n",
    "    #RecallScore = average of individual Go Term recall scores\n",
    "    RecallScore = sum(RecallPerGoTerm)/len(RecallPerGoTerm) #denominator is non-zero\n",
    "    PrecisionScore = sum(PrecisionPerGoTerm)/len(PrecisionPerGoTerm)\n",
    "    return RecallScore, PrecisionScore\n",
    "\n",
    "    \n",
    "def F_score(predicted, actuals, method = 'GoTerm'):\n",
    "    \"\"\"\n",
    "    @param predicted: data type = Variable\n",
    "    @param actuals: data type = Variable\n",
    "    @return: Maximum f score over all values of tau and the corresponding tau threshold\n",
    "    \"\"\"\n",
    "    f_max, optimal_threshold, optimal_precision, optimal_recall = 0, 0, 0, 0\n",
    "    for threshold in [i/100 for i in range(1,100)]:\n",
    "        predicted_tau = round_manual(predicted.data.numpy(), threshold)\n",
    "        \n",
    "        if method == 'GoTerm':\n",
    "            recall_score, precision_score = recall_precision_GoTermMethod(predicted_tau, actuals)\n",
    "        elif method == 'Protein':\n",
    "            recall_score, precision_score = recall_precision_ProteinMethod(predicted_tau, actuals)\n",
    "        \n",
    "        if recall_score==0 and precision_score==0:\n",
    "            output = 0\n",
    "        else:\n",
    "            output = ((2*precision_score*recall_score) / (precision_score + recall_score))\n",
    "        if output > f_max:\n",
    "            f_max = output\n",
    "            optimal_threshold = threshold\n",
    "            optimal_precision = precision_score\n",
    "            optimal_recall = recall_score\n",
    "    \n",
    "    return f_max, optimal_threshold, optimal_precision, optimal_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Early stop condition and training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def early_stop(val_loss_history, t=3, required_progress=0.001):\n",
    "    \"\"\"\n",
    "    Stop the training if there is no non-trivial progress in k steps\n",
    "    @param val_acc_history: a list contains all the historical validation acc\n",
    "    @param required_progress: the next acc should be higher than the previous by \n",
    "        at least required_progress amount to be non-trivial\n",
    "    @param t: number of training steps \n",
    "    @return: a boolean indicates if the model should earily stop\n",
    "    \"\"\"\n",
    "    # TODO: add your code here\n",
    "    \n",
    "    cnt = 0 # initialize the count --> to store count of cases where difference in\n",
    "                                    #  accuracy is less than required progress.\n",
    "    \n",
    "    if(len(val_loss_history) > 0): # if list has size > 0 \n",
    "        for i in range(t): # start the loop\n",
    "            index = len(val_loss_history) - (i+1) # start from the last term in list and move to the left\n",
    "            if (index >= 1): # to check if index != 0 --> else we can't compare to previous value\n",
    "                if ((val_loss_history[index-1] - val_loss_history[index]) < required_progress):\n",
    "                    cnt += 1 # increase the count value\n",
    "                else:\n",
    "                    break # break if difference is greater \n",
    "    \n",
    "    if(cnt != t): # if count is equal to t, return True\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "    \n",
    "def train_test(valid_sequences, valid_label, valid_length, num_epochs, optimizer, data_iter, model, training_length, method = 'GoTerm'):\n",
    "    losses = []\n",
    "    total_batches = int(training_length/ batch_size)\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        stop_training = False\n",
    "        for i, (train_data, train_labels, length_batch) in enumerate(data_iter):\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs = model(train_data, length_batch)\n",
    "\n",
    "            loss = criterion(outputs, train_labels.float())\n",
    "            losses.append(loss.data[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            '''\n",
    "            May want to re-consider whether we need to find validation metrics after each step.\n",
    "            For FastText, not a major issue.\n",
    "            '''\n",
    "            \n",
    "            model.eval()\n",
    "            val_outputs = model(Variable(valid_sequences), Variable(valid_length))\n",
    "            val_loss = criterion(val_outputs, Variable(valid_label).float()).data[0]\n",
    "            validation_loss_history.append(val_loss)\n",
    "            f_score,threshold,precision,recall = F_score(val_outputs, valid_label, method = method)\n",
    "            stop_training = early_stop(validation_loss_history)\n",
    "\n",
    "            if stop_training:\n",
    "                print(\"earily stop triggered\")\n",
    "                break\n",
    "            \n",
    "            if (i+1) % batch_size == 0:\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train loss: {}, \\nF_Score: {}, Threshold: {}, Precision: {}, Recall: {}, Validation Loss: {}'\\\n",
    "                      .format(epoch, num_epochs, i+1, total_batches, np.mean(losses)/(total_batches*epoch), \\\n",
    "                        f_score, threshold, precision,recall, val_loss))\n",
    "        \n",
    "        if stop_training == True:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Prediction Test Data performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FScore_on_test_set(model, test_input_seq, test_seq_length, test_output_labels, num_labels):\n",
    "    test_input_seq = Variable(test_input_seq)\n",
    "    test_seq_length = Variable(test_seq_length)\n",
    "    predicted = model(test_input_seq, test_seq_length)\n",
    "    fmax, optimal_threshold, optimal_precision, optimal_recall = F_score(predicted, test_output_labels)\n",
    "    return fmax, optimal_threshold, optimal_precision, optimal_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Model training and test performance - FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "vocab_size = 26 # number words in the vocabulary base\n",
    "emb_dim = 20 # dimension for n-gram embedding\n",
    "num_epochs = 500 # number epoch to train\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART I - Human Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_size = len(human_train_sequences)\n",
    "num_labels = human_GO_terms.shape[0] #147\n",
    "\n",
    "model = FastText(vocab_size, emb_dim, num_labels)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brenton/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:66: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/500], Step: [20/487], Train loss: 0.0018586146329707435, \n",
      "F_Score: 0.09317472083276401, Threshold: 0.44, Precision: 0.05153771088373854, Recall: 0.48501735943922947, Validation Loss: 0.8997994065284729\n",
      "Epoch: [1/500], Step: [40/487], Train loss: 0.0017362236762438466, \n",
      "F_Score: 0.09376158754723929, Threshold: 0.39, Precision: 0.05262143030328692, Recall: 0.4297318615056567, Validation Loss: 0.8597059845924377\n",
      "earily stop triggered\n",
      "Test Data F-Score for yeast protein prediction is 0.0496417219099 \n",
      "Precision: 0.0259645771195 \n",
      "Recall: 0.563481725967 \n",
      "Threshold: 0.37\n"
     ]
    }
   ],
   "source": [
    "data_iter = batch_iter(human_train_vectors, human_train_labels, human_train_lengths, batch_size)\n",
    "\n",
    "# Model Training\n",
    "train_test(human_valid_vectors, human_valid_labels, human_valid_lengths, \\\n",
    "           num_epochs, optimizer, data_iter, model, data_size, method = 'GoTerm') \n",
    "\n",
    "FScore,Threshold,Precision,Recall = FScore_on_test_set(model, human_test_vectors, human_test_lengths,human_test_labels, num_labels)\n",
    "\n",
    "# Prediction on test set\n",
    "print(\"Test Data F-Score for yeast protein prediction is\", FScore, '\\nPrecision:',Precision,\n",
    "      '\\nRecall:',Recall, '\\nThreshold:', Threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART II - Yeast Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_size = len(yeast_train_sequences) #3447\n",
    "num_labels = yeast_GO_terms.shape[0] #26\n",
    "model = FastText(vocab_size, emb_dim, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Brenton/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:66: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earily stop triggered\n",
      "Test Data F-Score for yeast protein prediction is 0.178014263906 \n",
      "Precision: 0.0999140602339 \n",
      "Recall: 0.815359165688 \n",
      "Threshold 0.49\n"
     ]
    }
   ],
   "source": [
    "data_iter = batch_iter(yeast_train_vectors, yeast_train_labels, yeast_train_lengths, batch_size)\n",
    "\n",
    "# Model Training\n",
    "train_test(yeast_valid_vectors, yeast_valid_labels, yeast_valid_lengths, \\\n",
    "           num_epochs, optimizer, data_iter, model, data_size, method = 'GoTerm')\n",
    "\n",
    "FScore,Threshold,Precision,Recall = FScore_on_test_set(model, yeast_test_vectors, yeast_test_lengths,yeast_test_labels, num_labels)\n",
    "\n",
    "# Prediction on test set\n",
    "print(\"Test Data F-Score for yeast protein prediction is\", FScore, '\\nPrecision:',Precision,\n",
    "      '\\nRecall:',Recall, '\\nThreshold', Threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
