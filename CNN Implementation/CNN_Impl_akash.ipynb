{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training prots: 3447\n",
      "Number of validation prots: 963\n",
      "Number of testing prots: 206\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from Bio import SeqIO\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from evaluation_metrics import *\n",
    "from data_processing import *\n",
    "\n",
    "#Yeast sequences\n",
    "fasta = '../../../data/yeast_sequences.fasta'\n",
    "test_set_file = '../../../data/yeast_MF_temporal_holdout.mat'\n",
    "\n",
    "sequences, names = load_FASTA(fasta)\n",
    "train_inds, valid_inds, test_inds, y_trainYeast, y_validYeast, y_testYeast, go_termsYeast = load_test_sets(test_set_file)\n",
    "\n",
    "train_seqsYeast = [sequences[i] for i in train_inds]\n",
    "print('Number of training prots: ' + str(len(train_seqsYeast)))\n",
    "valid_seqsYeast = [sequences[i] for i in valid_inds]\n",
    "print('Number of validation prots: ' + str(len(valid_seqsYeast)))\n",
    "test_seqsYeast = [sequences[i] for i in test_inds]\n",
    "print('Number of testing prots: ' + str(len(test_seqsYeast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yTrainYeast = torch.from_numpy(y_trainYeast).type(torch.LongTensor)\n",
    "yValidYeast = torch.from_numpy(y_validYeast).type(torch.LongTensor)\n",
    "yTestYeast = torch.from_numpy(y_testYeast).type(torch.LongTensor)\n",
    "\n",
    "# yTrainYeast = torch.from_numpy(y_trainYeast).type(torch.LongTensor)\n",
    "# yValidYeast = torch.from_numpy(y_validYeast).type(torch.LongTensor)\n",
    "# yTestYeast = torch.from_numpy(y_testYeast).type(torch.LongTensor)\n",
    "\n",
    "#yTrainYeast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Length of train, valid and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 1 #value for kmers\n",
    "\n",
    "# train_seqsYeast_length = sequence_lengths_with_kmers(train_seqsYeast, k) \n",
    "# valid_seqsYeast_length = sequence_lengths_with_kmers(valid_seqsYeast, k)\n",
    "# test_seqsYeast_length = sequence_lengths_with_kmers(test_seqsYeast, k)\n",
    "train_seqsYeast_length = sequence_lengths_with_kmers(train_seqsYeast, k)\n",
    "valid_seqsYeast_length = sequence_lengths_with_kmers(valid_seqsYeast, k)\n",
    "test_seqsYeast_length = sequence_lengths_with_kmers(test_seqsYeast, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize all amino-acid chains in the list \n",
    "#### Each amino-acid string becomes one row in a tensor object.\n",
    "#### This tensor object has dimension NxD, where N is the number of amino-acid strings and D is the length of the longest chain in the set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get kmers (Can take upto 3-4 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if k==1:\n",
    "    k_mers_yeast = None\n",
    "else:\n",
    "    k_mers_yeast = get_k_mers(train_seqsYeast, valid_seqsYeast, test_seqsYeast, k, org=\"yeast\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runs quickly for Yeast, about 2 minutes for Yeast data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "TrainSeqsYeast = TransformAAsToTensor_with_kmers(train_seqsYeast, k, k_mers_yeast, acid_dict_yeast)\n",
    "ValidSeqsYeast = TransformAAsToTensor_with_kmers(valid_seqsYeast, k, k_mers_yeast, acid_dict_yeast)\n",
    "TestSeqsYeast = TransformAAsToTensor_with_kmers(test_seqsYeast, k, k_mers_yeast, acid_dict_yeast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "#vocab_size = 26 # number words in the vocabulary base\n",
    "num_labels = go_termsYeast.shape[0] \n",
    "if k == 1:\n",
    "    vocab_size = len(acid_dict_yeast) + 1\n",
    "else:\n",
    "    vocab_size = max(list(k_mers_yeast.values())) + 1\n",
    "emb_dim = 50 # dimension for n-gram embedding\n",
    "num_epochs = 5 # number epoch to train\n",
    "batch_size = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Get batch data method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) GRU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN model\n",
    "    \"\"\"\n",
    "       \n",
    "    def __init__(self, vocab_size, emb_dim, num_labels, hidden_size, n_layers=1, dropout=0.1, is_bidirectional = False):\n",
    "       \n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(CNN,self).__init__()\n",
    "        Ci = 1\n",
    "        Co = 2\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.conv13 = nn.Conv2d(Ci, Co, (3, emb_dim))\n",
    "        self.conv14 = nn.Conv2d(Ci, Co, (4, emb_dim))\n",
    "        self.conv15 = nn.Conv2d(Ci, Co, (5, emb_dim))\n",
    "        self.convs = [self.conv13, self.conv14, self.conv15]\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(3*Co, num_labels) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) \n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x) # (N,len(Ks)*Co)\n",
    "        logit = self.fc1(x) # (N,C)\n",
    "        #return logit\n",
    "        return torch.nn.functional.softmax(logit)\n",
    "        \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def early_stop(val_acc_history, t=2, required_progress=0.00001):\n",
    "    \"\"\"\n",
    "    Stop the training if there is no non-trivial progress in k steps\n",
    "    @param val_acc_history: a list contains all the historical validation acc\n",
    "    @param required_progress: the next acc should be higher than the previous by \n",
    "        at least required_progress amount to be non-trivial\n",
    "    @param t: number of training steps \n",
    "    @return: a boolean indicates if the model should earily stop\n",
    "    \"\"\"\n",
    "    # TODO: add your code here\n",
    "    \n",
    "    cnt = 0 # initialize the count --> to store count of cases where difference in\n",
    "                                    #  accuracy is less than required progress.\n",
    "    \n",
    "    if(len(val_acc_history) > 0): # if list has size > 0 \n",
    "        for i in range(t): # start the loop\n",
    "            index = len(val_acc_history) - (i+1) # start from the last term in list and move to the left\n",
    "            if (index >= 1): # to check if index != 0 --> else we can't compare to previous value\n",
    "                if (abs(val_acc_history[index] - val_acc_history[index-1]) < required_progress):\n",
    "                    cnt += 1 # increase the count value\n",
    "                else:\n",
    "                    break # break if difference is grea-ter \n",
    "    \n",
    "    if(cnt != t): # if count is equal to t, return True\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def train(valid_sequences, valid_label, num_epochs, optimizer, data_iter, model, training_length, threshold):\n",
    "    losses = []\n",
    "    total_batches = int(training_length/ batch_size) #375\n",
    "\n",
    "    eval_every = 10\n",
    "    print_every = 10\n",
    "    validate_every = int((eval_every/100)*total_batches)\n",
    "    show_every = int((print_every/100)*total_batches)\n",
    "    val_outputs = None\n",
    "    eval_loss = None\n",
    "    valid_label = None\n",
    "    validation_losses = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        stop_training = False\n",
    "        for i, (train_data, train_labels, length_batch) in enumerate(data_iter):\n",
    "                                                # train_data size: (26, 34350) ; train_label size: (26, 147)\n",
    "                                                # This needs to be modified. Max length is batch specific !!!!!\n",
    "            model.train(True)\n",
    "            model.zero_grad()\n",
    "            outputs = model(train_data)\n",
    "            loss = criterion(outputs, train_labels.float())\n",
    "            losses.append(loss.data[0])\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            clipped = torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "            # clip gradients because RNN\n",
    "            for pr in model.parameters():\n",
    "                pr.data.add_(-clipped, pr.grad.data)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1)%validate_every == 0:\n",
    "                # Erly stop using validation loss\n",
    "                valid_sequences, valid_label = reduced_set(ValidSeqsYeast, valid_seqsYeast_length, yValidYeast, 100)\n",
    "\n",
    "                model.eval()\n",
    "                val_outputs = model(Variable((valid_sequences).type(torch.LongTensor), volatile=True))\n",
    "                eval_loss = criterion(val_outputs.cpu().data, valid_label.float())\n",
    "                print(eval_loss.data[0])\n",
    "                validation_losses.append(eval_loss.data[0])\n",
    "                stop_training = early_stop(validation_losses, 5)\n",
    "\n",
    "            # Print statements\n",
    "            if stop_training:\n",
    "                print(\"earily stop triggered\")\n",
    "                break\n",
    "            if (i+1) % show_every == 0:\n",
    "                print('Epoch: [{0}/{1}], Step: [{2}/{3}], Train loss: {4}, Validation loss:{5}'.format(\n",
    "                           epoch, num_epochs, i+1, total_batches, np.mean(losses)/(total_batches*epoch), np.mean(np.array(validation_losses))))\n",
    "                \n",
    "        #evaluate_and_save(model, opt.out_dir, val_outputs, valid_label, losses, eval_loss.data[0], \"yeast\", epoch)\n",
    "        if stop_training == True:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_size = len(train_seqsYeast) #9751\n",
    "num_labels = go_termsYeast.shape[0] #147\n",
    "hidden_size = 50\n",
    "eval_every = 2\n",
    "print_every = 5\n",
    "\n",
    "model = CNN(vocab_size, 50, num_labels, hidden_size, n_layers=1, dropout=0.1, is_bidirectional=False)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7043743133544922\n",
      "Epoch: [1/1], Step: [13/132], Train loss: 0.005372096921180512, Validation loss:0.7043743133544922\n",
      "0.7025437951087952\n",
      "Epoch: [1/1], Step: [26/132], Train loss: 0.005367826297010853, Validation loss:0.7034590542316437\n",
      "0.7013904452323914\n",
      "Epoch: [1/1], Step: [39/132], Train loss: 0.005365214911929934, Validation loss:0.7027695178985596\n",
      "0.7007626891136169\n",
      "Epoch: [1/1], Step: [52/132], Train loss: 0.0053632017615791805, Validation loss:0.7022678107023239\n",
      "0.7012298107147217\n",
      "Epoch: [1/1], Step: [65/132], Train loss: 0.005363278747438551, Validation loss:0.7020602107048035\n",
      "0.7013167142868042\n",
      "Epoch: [1/1], Step: [78/132], Train loss: 0.005362049937711359, Validation loss:0.701936294635137\n",
      "0.701301097869873\n",
      "Epoch: [1/1], Step: [91/132], Train loss: 0.005360435817267869, Validation loss:0.7018455522400993\n",
      "0.7010495662689209\n",
      "Epoch: [1/1], Step: [104/132], Train loss: 0.005358951535327729, Validation loss:0.7017460539937019\n",
      "0.7006999850273132\n",
      "Epoch: [1/1], Step: [117/132], Train loss: 0.005358271426223344, Validation loss:0.7016298241085477\n",
      "0.7005912661552429\n",
      "Epoch: [1/1], Step: [130/132], Train loss: 0.005357311315191932, Validation loss:0.7015259683132171\n"
     ]
    }
   ],
   "source": [
    "data_iter = batch_iter(batch_size, TrainSeqsYeast, yTrainYeast, train_seqsYeast_length)\n",
    "\n",
    "num_epochs = 1\n",
    "threshold = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 1]\n",
    "\n",
    "# Model Training\n",
    "ValidSeqsYeast_small, yValidYeast_small = reduced_set(ValidSeqsYeast, valid_seqsYeast_length, yValidYeast, 100)\n",
    "train(ValidSeqsYeast_small, yValidYeast_small, num_epochs, optimizer, data_iter, model, data_size, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Testing performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_set_predictions(model, test_input_seq):\n",
    "    model.eval()\n",
    "    test_input_seq = Variable(test_input_seq)\n",
    "    predicted = model(test_input_seq.transpose(0,1).type(torch.LongTensor))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predictions = test_set_predictions(model, TestSeqsYeast)\n",
    "#FScore, Threshold, Precision, Recall = F_score(test_set_predictions, yTestYeast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_AUPR_curve(test_predictions.data.numpy(), yTestYeast.numpy(), label='GRU', org='Yeast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_AUC_curve(test_predictions.data.numpy(), yTestYeast.numpy(), label='GRU', org='Yeast')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
